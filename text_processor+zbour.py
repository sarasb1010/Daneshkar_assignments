# -*- coding: utf-8 -*-
"""text_processor+ZBour.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g8AVgMornaiwznE6r2A9r8LPiREX_zvL
"""

# text_processor.py  Z.Bour


import re
import json
import string
from collections import Counter

class TextProcessor:
    def __init__(self, file_path, output_json):
        """ Initialize variables """
        self.file_path = file_path
        self.output_json = output_json
        self.stop_words = set([
            "و", "در", "به", "از", "که", "این", "را", "با", "است", "برای",
            "آن", "یک", "هم", "تا", "نیز", "اما", "یا", "بر", "اگر", "هر",
            "چون", "باید", "می", "شد", "کند", "کرد", "شده", "دیگر", "همه",
            "نیک", "اینجا", "اینها", "آنان", "خود"
        ])

    def read_file(self):
        """ Read content from the text file """
        with open(self.file_path, 'r', encoding='utf-8') as file:
            return file.read()

    def clean_text(self, text):
        """ Clean the text: remove emails, URLs, and punctuation """
        # Remove emails
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        # Remove punctuation
        punctuation_pattern = r'[{}]'.format(re.escape(string.punctuation + '«»؛؟،٫٬'))
        text = re.sub(punctuation_pattern, '', text)
        return text

    def remove_stopwords(self, words):
        """ Remove common stop words """
        return [word for word in words if word not in self.stop_words]

    def count_word_frequencies(self, words):
        """ Count the frequency of each word """
        return dict(Counter(words))

    def save_to_json(self, word_counts):
      """ Save the word frequencies to two JSON files:
          1. Unsorted dictionary
          2. Sorted list from most to least frequent
      """
      # Save as a regular dictionary
      with open(self.output_json, 'w', encoding='utf-8') as f:
          json.dump(word_counts, f, ensure_ascii=False, indent=4)

      # Save sorted frequencies (most to least frequent)
      sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)
      sorted_output_file = self.output_json.replace(".json", "_sorted.json")
      with open(sorted_output_file, 'w', encoding='utf-8') as f:
          json.dump(sorted_counts, f, ensure_ascii=False, indent=4)



    def process(self):
        """ Process the text through all the stages """
        text = self.read_file()  # Step 1: Read the file
        cleaned_text = self.clean_text(text)  # Step 2: Clean the text
        words = cleaned_text.split()  # Step 3: Tokenize into words
        filtered_words = self.remove_stopwords(words)  # Step 4: Remove stopwords
        word_counts = self.count_word_frequencies(filtered_words)  # Step 5: Count frequencies
        self.save_to_json(word_counts)  # Step 6: Save the results
        print(f"Processing complete! Output saved to {self.output_json}")


# Example usage
if __name__ == "__main__":
    processor = TextProcessor("input.txt", "word_frequencies.json")
    processor.process()